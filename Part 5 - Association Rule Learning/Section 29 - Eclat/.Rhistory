install.packages(c('caret','data.table'))
install.packages(c('doParallel','DT','e1071','ggplot2','ggthemes','gridExtra','lubridate','mltools','plyr','dplyr','readxl','reshape2','ROCR','scales','shiny','shinydashboard','shinyjs','tidyr','xlsx','zoo','randomForest'))
install.packages(c("doParallel", "DT", "e1071", "ggplot2", "ggthemes", "gridExtra", "lubridate", "mltools", "plyr", "dplyr", "readxl", "reshape2", "ROCR", "scales", "shiny", "shinydashboard", "shinyjs", "tidyr", "xlsx", "zoo", "randomForest"))
shiny::runApp('Z:/2.B2B Pricing Solution/4. Working Folder/Codes/CheesePricingB2BSimulator - Sandbox')
library(rJava)
runApp('Z:/2.B2B Pricing Solution/4. Working Folder/Codes/CheesePricingB2BSimulator - Sandbox')
i = 1
a = c(1,2,3)
paste0(i,a)
paste0(c(i), a)
dmv_final <- read.csv("C:/Users/sridevi.tolety/Documents/9.FC_Demand_Forecasting/4. Working Folder/3.OutputFiles/22May2018_dmv/dmv_Final.csv")
unique(dmv_final$SalesOrg)
class(dmv_final$SalesOrg)
dmv_final2 <- dmv_final[!dmv_final$SalesOrg==5030,]
dmv_final <- dmv_final[!dmv_final$SalesOrg==5030,]
exclude <- c('6398400','6477100','6250100','6453600','6350600','6380800','6410700','6575800','6436300','6400900','6411100','6135800','651200','6452500','230038','6395700','6446100','6575700','6132200','666948','236207','230028','230013','659329','6575500','669682','6101700','6448600','6391000','821484','659093','230016','653309','653311','653321','659722','661240','661241','653347','6100331','6100487','6100903','6100910','6100926','653348','664707','6100791','653318','653322','6100932','6100913','653314','653306','6100261','6100494','663413','293716','598227','661010','733521','653349','666302','596667','732876','746197','659732','748421','666836','734171','748102','666954','651105','734135','293723','598715','6100904','660285','587788','747459')
names(dmv_final)
dmv_final2 <- dmv_final[!dmv_final$Material %in% exclude,]
dmv_final <- dmv_final[!dmv_final$Material %in% exclude,]
dmv_Final_tillNov_forJanFeb <- dmv_final[!dmv_final$Date %in% c("12/1/2017","1/1/2018","2/1/2018","3/1/2018","4/1/2018"),]
dmv_Final_tillDec_forMar <- dmv_final[!dmv_final$Date %in% c("1/1/2018","2/1/2018","3/1/2018","4/1/2018"),]
dmv_Final_tillJan_forApr <- dmv_final[!dmv_final$Date %in% c("2/1/2018","3/1/2018","4/1/2018"),]
write.csv(dmv_Final_tillNov_forJanFeb, "C:/Users/sridevi.tolety/Documents/9.FC_Demand_Forecasting/4. Working Folder/3.OutputFiles/22May2018_DMV/dmv_Final_tillNov_forJanFeb.csv", row.names = F)
write.csv(dmv_Final_tillDec_forMar, "C:/Users/sridevi.tolety/Documents/9.FC_Demand_Forecasting/4. Working Folder/3.OutputFiles/22May2018_DMV/dmv_Final_tillDec_forMar.csv", row.names = F)
write.csv(dmv_Final_tillJan_forApr, "C:/Users/sridevi.tolety/Documents/9.FC_Demand_Forecasting/4. Working Folder/3.OutputFiles/22May2018_DMV/dmv_Final_tillJan_forApr.csv", row.names = F)
write.csv(dmv_Final, "C:/Users/sridevi.tolety/Documents/9.FC_Demand_Forecasting/4. Working Folder/3.OutputFiles/22May2018_DMV/dmv_Final.csv", row.names = F)
write.csv(dmv_final, "C:/Users/sridevi.tolety/Documents/9.FC_Demand_Forecasting/4. Working Folder/3.OutputFiles/22May2018_DMV/dmv_Final.csv", row.names = F)
write.csv(dmv_Final_tillJan_forApr, "C:/Users/sridevi.tolety/Documents/9.FC_Demand_Forecasting/4. Working Folder/3.OutputFiles/22May2018_DMV/dmv_Final_tillJan_forApr.csv", row.names = F)
# Tom's 6m lag validation exercise
prices <- read.csv("C:/Users/sridevi.tolety/Documents/11.FC_Apollo_Caseinate/4. Working Directory/Data/DataForTomsComparison.csv")
train <- prices[prices$Date<201107,]
train <- prices[prices$Date<201707,]
test <- prices[prices$Date>201706,]
mod_lag6 <- lm(Caseinate_EM7_EMEA~PV_i_HCQAct_Lag6, data=train)
mod_lag6
test$pred <- predict(mod_lag6, newdata = test)
View(test)
View(prices)
View(test)
prices$Date <- paste0("01-",prices$Month,"-",prices$Year)
prices$Date <- as.Date(prices$Date, "%d-%m-%Y")
train <- prices[prices$Date<201707,]
test <- prices[prices$Date>201706,]
train <- prices[prices$Date<'2017-07-01',]
test <- prices[prices$Date>'2017-06-01',]
mod_lag6 <- lm(Caseinate_EM7_EMEA~PV_i_HCQAct_Lag6, data=train)
mod_lag6
test$pred <- predict(mod_lag6, newdata = test)
test$APE <- abs(test$Caseinate_EM7_EMEA-test$pred)/test$Caseinate_EM7_EMEA
mean(test$APE)
round(mean(test$APE)*100,2)
test$APE <- round(abs(test$Caseinate_EM7_EMEA-test$pred)*100/test$Caseinate_EM7_EMEA,2)
round(mean(test$APE)*100,2)
round(mean(test$APE),2)
source('~/2.B2B Pricing Solution/4. Working Folder/Codes/CheesePricingB2BSimulator - Jun2018/Jun2018_Code.R')
# Association Rule Learning  ----------------------------------------------
dataset = read.csv('Market_Basket_Optimisation.csv', header = FALSE)
setwd("~/StudyReferences/MachineLearning_AtoZ_Udemy/Part 5 - Association Rule Learning")
data <- read_excel("C:/Users/sridevi.tolety/Documents/Fractal Files/ABInBev/DATA_COMMERCIAL_AGREEMENTS.xlsx")
library(readxl)
data <- read_excel("C:/Users/sridevi.tolety/Documents/Fractal Files/ABInBev/DATA_COMMERCIAL_AGREEMENTS.xlsx")
View(data)
# Each product : 1 column
library(arules)
dataset = read.transactions('Market_Basket_Optimisation.csv', sep=',', rm.duplicates = TRUE)
summary(dataset)
setwd("~/StudyReferences/MachineLearning_AtoZ_Udemy/Part 5 - Association Rule Learning/Section 28 - Apriori")
# Each product : 1 column
library(arules)
dataset = read.transactions('Market_Basket_Optimisation.csv', sep=',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN=10)
# Training apriori on the dataset
rules = apriori(data=dataset,
parameter = list(support=(7*3/7500), confidence=0.8)) # This support sets a minimum support for at least 3 purchases a day, 7 times a wekk divided by total number of transactions
rules = apriori(data=dataset,
parameter = list(support=(7*3/7500), confidence=0.4))
# Visualizing the results
inspect(sort(rules, by='lift')[1:10])
0.002866/0.40277
# Reduce the confidence further
rules = apriori(data=dataset,
parameter = list(support=(7*3/7500), confidence=0.2))
inspect(sort(rules, by='lift')[1:10])
# Suppose we want items that are purchased at least three times a day.
# Thats 3*7 times a week
# Support = 3*7 / 7500 = 0.0037333 ~ 0.004
rules = apriori(data=dataset,
parameter = list(support=(0.004), confidence=0.2))
inspect(sort(rules, by='lift')[1:10])
setwd("~/StudyReferences/MachineLearning_AtoZ_Udemy/Part 5 - Association Rule Learning/Section 29 - Eclat")
dataset = read.csv('Market_Basket_Optimisation.csv', header = FALSE)
# Each product : 1 column
library(arules)
dataset = read.transactions('Market_Basket_Optimisation.csv', sep=',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN=10)
# Training Eclat on the dataset
rules = eclat(data=dataset,
parameter = list(support=0.004, minlen = 2))
# Visualizing the results
inspect(sort(rules, by='lift')[1:10])
# Visualizing the results
inspect(sort(rules, by='support')[1:10])
